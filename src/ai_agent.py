from llm_interface import LLMClient

class AIAgent:
    """
    AIAgent class is designed to process a series of steps defined in a configuration to perform reasoning tasks.
    Each step involves generating a prompt based on a template and inputs, and then using an LLM client to process the prompt.
    
    Attributes:
        config (dict): Configuration for the AI agent, including steps for reasoning and system-wide prompts.
    """
    
    def __init__(self, steps_config: dict[str, any]):
        """
        Initializes the AIAgent with a configuration for its reasoning process.
        
        Args:
            steps_config (dict): The configuration dict containing the steps for reasoning and other necessary data.
        """
        self.config = steps_config
    
    def reasoning(self, input_collection: dict[str, any], llmClient: 'LLMClient') -> str:
        """
        Processes each step defined in the agent's configuration, using inputs from the input collection and 
        generating outputs via the LLM client. Each step's output is added back into the input collection for use in subsequent steps.
        
        Args:
            input_collection (dict): A dictionary of inputs available for processing the steps.
            llmClient (LLMClient): An instance of a client capable of interfacing with a Large Language Model (LLM) to process prompts.
            
        Returns:
            str: The final output generated by the reasoning process, as identified by 'final-output' in the input collection.
            
        Raises:
            AttributeError: If a required input for a step is not available in the input collection.
        """
        for step in self.config['steps']:
            # Print the current step being processed
            print(step['name'], "...")
            
            # Determine the system prompt to use for the current step
            system_prompt = step["system-prompt"] if "system-prompt" in step.keys() \
                else self.config['system-prompt']
            
            # Start forming the prompt based on the step's instruction template
            prompt = step["instruction"]
            
            # Replace placeholders in the prompt with actual inputs from the input collection
            for input_needed in step['inputs']:
                if input_needed not in input_collection.keys():
                    raise AttributeError(f"{input_needed} input is not available.")
                print(f"Processing {input_needed}...")
                prompt = prompt.replace(f"<{input_needed}>", input_collection[input_needed])
            
            # Query the LLM client with the constructed prompt and system role messages
            response = llmClient.ask_question(prompt=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ])
            
            # Update the input collection with the response from the LLM client for the current step's output
            input_collection[step['output']] = response
        
        # Return the final output from the input collection after all steps have been processed
        return input_collection['final-output']

